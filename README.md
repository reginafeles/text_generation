# Генерация текста
## Дано

1. Произвольный текст или языковой профиль на английском языке
2. Необходимо построить модель генерации текста

## Алгоритм

1. Из контекста берём последовательность из `n-1` - в нашем случае это последние две буквы: `('t', 'h')`;
2. Находим все три-граммы, которые начинаются с `('t', 'h')` (у нас их две);
3. Смотрим, для какой из три-грамм частота больше:
   * `F('t', 'h', 'e') > F('t', 'h', 'o')` 
4. Берём последнее слово из три-граммы с наибольшей частотой появления и добавляем в выходной текст;
   * В нашем случае берём `e` и добавляем к выходному тексту, получается следующее: `I am the`.
5. Повторяем шаги 1-4 при дальнейшей генерации.

### Шаг 1. Токенизировать заданную последовательность

### Шаг 2. Подготовить хранилище соответствий буква-число

### Шаг 2.1. Переопределить родительский метод заполнения хранилища
 метод `update` класса `LetterStorage` принимает
на вход токенизированный корпус из элементов-букв.
Метод заполняет хранилище заданными буквами.
В случае возникновения ошибки, метод возвращает `-1`.

### Шаг 2.2. Добавить новый метод в хранилище букв

Метод возвращает количество букв в хранилище на момент
вызова метода.

Если хранилище пустое, то метод возвращает значение `-1`.

### Шаг 3. Кодировать последовательность 

Например, для заданного токенизированного корпуса:
```py
(
    ('_', 's', 'h', 'e', '_'), ('_', 'i', 's', '_'), 
    ('_', 'h', 'a', 'p', 'p', 'y', '_'), ('_', 'h', 'e', '_'), 
    ('_', 'i', 's', '_'), ('_', 'h', 'a', 'p', 'p', 'y', '_')
)
``` 
Будет получен следующий закодированный корпус:
```py
(
    (1, 2, 3, 4, 1), (1, 5, 2, 1), 
    (1, 3, 6, 7, 7, 8, 1), (1, 3, 4, 1), 
    (1, 5, 2, 1), (1, 3, 6, 7, 7, 8, 1)
)
```

В случае некорректного ввода функция возвращает пустой кортеж.

### Шаг 4. Декодировать последовательность 


Например, для заданного токенизированного корпуса:
```py
(
    (1, 2, 3, 4, 1), (1, 5, 2, 1), 
    (1, 3, 6, 7, 7, 8, 1), (1, 3, 4, 1), 
    (1, 5, 2, 1), (1, 3, 6, 7, 7, 8, 1)
)
``` 
Будет получен следующий декодированный корпус:
```py
(
    ('_', 's', 'h', 'e', '_'), ('_', 'i', 's', '_'), 
    ('_', 'h', 'a', 'p', 'p', 'y', '_'), ('_', 'h', 'e', '_'), 
    ('_', 'i', 's', '_'), ('_', 'h', 'a', 'p', 'p', 'y', '_')
)
```

В случае некорректного ввода функция возвращает пустой кортеж.


### Шаг 5. Продемонстрировать обработку большого текста


### Шаг 6. Импортировать модуль с языковым профилем для модели генерации текста
класс `LanguageProfile` из модуля `language_profile.py`.

### Шаг 7. Объявить сущность модели генерации текста

Идея генерации текста с помощью N-грамм состоит в следующем: 
последняя буква в N-грамме может быть найдена исходя из 
предшествующих ей букв. 

Модели необходимо следующее:
 * Собранный языковой профиль на определенном языке 

### Шаг 7.1. Генерация следующей буквы

Для генерации буквы нам необходимо: 
 * Контекст (последние `n-1` букв);
 * Частотное распределение N-грамм в языке.

Самым простым способом для генерации следующей буквы на основе имеющихся данных будет следующий алгоритм:
1. Найти все N-граммы, которые начинаются с заданного контекста;
2. Выбрать из них ту, которая имеет наибольшую частоту встречаемости;
3. Взять последнюю букву из найденной N-граммы.

Если приходят некорректные значения (или некорректный ввод `context`, 
или его неправильный размер), то возвращается значение `-1`.

Если нет ни одной нграммы, которая бы соответствовала контексту, 
нужно найти самую частую нграмму среди всех нграмм, 
взять её последнюю букву и вернуть в виде предсказания.


**Примечание**: Поскольку наш алгоритм
берет наиболее частотные N-граммы, то ему
будет несложно постоянно выбирать одни и те же
частотные N-граммы при генерации как одной буквы,
так и нескольких слов или целого предложения.
Поэтому, во избежания выбора одних и тех же
вариантов предлагается убирать использованные
N-граммы. Генерация в таком случае становится
более осмысленной и менее повторяющейся.

**Пример**:

Есть модель с размером N-грамм равным 2, 
она уже обучена на тексте `'She is happy. He is happy.'`, 
выделены N-граммы, вычислены их частоты.
 
Дан контекст: `(3, )`. Пусть `3` соответствует букве `h`
(в вашей реализации может быть другой код). 
Необходимо по этому контексту предсказать следующую букву.

Находим все возможные N-граммы, которые начинаются с токенов из контекста, и их частоты:

```
(3, 4): 2,
(3, 6): 2
```

Выбираем из этих N-грамм ту, которая имеет наибольшую частоту - в данном случае `(3, 4)`
(две N-граммы с одинаковым контекстом, у обоих одинаковые частоты - берем первую). 
Токен `e` с идентификатором `4` и будет предсказанной буквой. 
В итоге получаем последовательность `he`. 
### Шаг 7.2. Генерация целого слова

Так как мы разбивали буквы и добавляли символ `_` для индикации начала и конца слов, 
у нас есть N-граммы, содержащие данный токен. Это позволяет нам выделить критерий, 
по которому генерация текста может останавливаться:
 * Если следующий сгенерированный токен = `_`, то происходит остановка генерации слова.
 * Если для заданного слова алгоритм не может найти конец, то генерацию предлагается
    остановить по задаваемому параметру `word_max_length`, который по умолчанию равен 15.
    


Если приходят некорректные значения, то возвращается пустой кортеж.

Если размер заданного контекста больше или равен параметру `word_max_length`,
то возвращается заданный контекст со вставленным специальным символом `'_'` в конце.

**Пример**:

Для текста `'She is happy. He is happy.'` и контекста `(3,)`,
будет сгенерировано следующее слово:
`(3, 4, 1)`, где `1` - идентификатор токена `_`, обозначающий 
начало и конец слова. В декодированном формате будет получено слово `he`.

### Шаг 7.3. Генерация предложения

Алгоритм генерации предложения можно представить следующим образом:
1. По заданному контексту происходит генерация слова
   1. Например для контекста `(1, 2)` было сгенерировано слово `(1, 2, 5, 6, 1)`.
2. Новый контекст образуется из последних `n-1` сгенерированных токенов  (включая токен `_`)
   1. То есть новый контекст для второго слова в нашем случае будет `(6, 1)`.
3. Шаги 1-2 повторяются для нового контекста.

В качестве возвращаемого значения будет кортеж из кортежей - закодированных слов.

Если приходят некорректные значения, то возвращается пустой кортеж.

**Пример**:

Есть языковой профиль `en_profile`, построенный на биграммах 
для текста `'She is happy. He is happy.'`. 
Дан контекст: `(3,)` и необходимое количество слов: `2`. 

Пусть идентификатор токена `_` равен `1`. 
Первое сгенерированное слово: `(3, 4, 1)`,
второе сгенерированное слово: `(1, 3, 6, 7, 7, 8, 1)`.

Сгенерированное предложения целиком:
`((3, 4, 1), (1, 3, 6, 7, 7, 8, 1))`. 
Сгенерированное предложение в декодированном формате:
`he happy`

### Шаг 8. Декодировать сгенерированную последовательность и перевести ее в текст

Функция принимает на вход декодированную последовательность
в виде кортежа кортежей. 

Функция возвращает строковую последовательность.

Правила построения строковой последовательности:
1. Два символа `_` подряд конвертируются в пробел.
2. Символ `_` в начале последовательности (если есть) 
    преобразуется в пустую строку.
3. Символ `_` в конце последовательности 
    преобразуется в завершающий знак `.`.
4. Первая буква в последовательности становится заглавной.

Например, декодированная последовательность:
`(('h', 'e', '_'), ('_', 'h', 'a', 'p', 'p', 'y', '_'))`
Сконвертируется в:
`He happy.`

### Шаг 9. Продемонстрировать работу генератора текста


### Шаг 10. Использование алгоритма расчета максимального правдоподобия для генерации слов

Вероятность последовательности букв рассчитывается следующим образом:

Это [*алгоритм максимального правдоподобия*](https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1), 
который позволяет расчитать вероятность появления буквы `w` после последовательности из `n-1` букв следующим образом:
   * Количество раз, которое буква `w` встретилось после последовательности из `n-1` / количество раз, которое последовательность `n-1` встретилась в корпусе.

Мы хотим лишь немного улучшить генерацию за счет алгоритма максимального 
правдоподобия. Так как и в случае с алгоритмом максимального правдоподобия 
генерация основывается на N-граммах, а также сохраняется интерфейс программы
для генерации, то логичнее всего сделать наследника и переопределить
нужные методы.


### Шаг 10.1. Использование алгоритма расчета максимального правдоподобия для генерации букв


**Пример**:

Есть модель с размером N-грамм равным 3, 
она уже обучена на некотором тексте, 
выделены N-граммы, вычислены их частоты. 

Дан контекст: `(1, 3)` и идентификатор `6`. 
Необходимо вычислить вероятность появления данного слова в данном контексте.

Мы находим частотность триграммы `(1, 3, 6)` - 
сколько раз данная триграмма встречалась в тексте.
Находим количество триграмм, которые начинаются с 
данного контекста - `(1, 3)`, берем сумму частот таких триграмм.

`n_gram_frequencies = {
    (1, 3, 6): 2,
    (1, 3, 4): 1
}`

Пусть частотность триграммы `(1, 3, 6)` равна `2`, 
а сумма частот у всех триграмм, начинающихся с `(1, 3)` равна `3`.
Тогда вероятность появления буквы с идентификатором `6` 
в данном контексте равняется отношению этих величин: `2/3`.

Если бы мы искали вероятность буквы с идентификатором `4`
в том же контексте `(1, 3)`, то мы бы получили значение `1/3`.

### Шаг 10.2. Генерация следующей буквы

Теперь необходимо переопределить генерацию следующей буквы 
с использованием алгоритма максимального правдоподобия, 
описанного в предыдущем шаге.

С использованием алгоритма максимального правдоподобия 
генерация слова будет выглядеть следующим образом:
1. Для каждого слова из имеющихся в словаре посчитать вероятность его
   появления после `n-1` слов - контекста;
2. Выбрать слово с наибольшей вероятностью.


Если приходят некорректные значения 
(или некорректный ввод `context`, или его неправильный размер), 
то возвращается значение `-1`.

Если данного контекста нет в словаре N-грамм, 
то возвращать необходимо наиболее частотную букву-униграмму

**Пример**:

Есть модель с размером N-грамм равным 3, 
она уже обучена на некотором тексте, 
выделены N-граммы, вычислены их частоты. 
Дан контекст: `(1, 3)`. Необходимо по этому 
контексту сгенерировать слово.

Частотное распределение триграмм следующее:
```py
{(1, 2, 3): 1, (2, 3, 4): 1, 
 (3, 4, 1): 2, (1, 5, 2): 2, 
 (5, 2, 1): 2, (1, 3, 6): 2, 
 (3, 6, 7): 2, (6, 7, 7): 2, 
 (7, 7, 8): 2, (7, 8, 1): 2, 
 (1, 3, 4): 1
}
```

Изначально, мы будем поочередно вызывать 
вспомогательный метод для подсчета максимального 
правдоподобия для контекста `(1, 3)` и букв 
с идентификаторами `6` и `4`. Максимальное
правдоподобие будет у триграммы `(1, 3, 6)` - `0.6[6]`,
тогда как у триграммы только `(1, 3, 4)` - `0.3[3]`.


### Шаг 11. Продемонстрировать работу генератора

### Шаг 12. Использование модели back-off N-грамм для генерации текста

Может возникнуть ситуация, когда контекста и/или 
буквы нет в хранилище N-грамм или в словаре соответственно. 
Это ведёт к тому, что у нас нет вероятностного распределения и, 
соответственно, мы не можем предсказать следующую букву.

Одним из способов решения данной проблемы является использование 
[алгоритма back-off](https://en.wikipedia.org/wiki/Katz's_back-off_model) 
в языковой модели N-грамм.

Суть заключается в следующем:
 * Если не была найдена N-грамма размера `n` - 
    то есть такого контекста для буквы `w` - происходит переход к модели языка, 
    которая была обучена с размером N-грамм = `n-1`. 
    И уже с помощью данных этой модели происходит предсказание;
 * Если же не было найдено N-грамм с размером `n-1`, 
    то происходит переход к модели с `n-2` - таким образом можно дойти до 
    `n = 1`, то есть до уни-грамм.
 * Чем большая размерность N-грамм участвует в генерации, тем точнее
    будет проходить генерация (например, генерация для триграмм будет 
    точнее и более правдоподобнее, чем для униграмм). С другой стороны,
    взяв слишком большой контекст, например 15-граммы, мы редко сможем
    найти необходимый контекст для генерации.
 * В случае, когда модель не может найти, например, 4-грамму для заданного
    контекста, при Back-off генерации, модель всегда может сузить этот контекст
    на одну букву и уже пытаться найти соответствующую 3-грамму (и так далее), 
    и тогда вероятность найти соответствующий контекст повышается, но с понижением
    размерности N-грамм из-за снижения размерности контекста снижается и точность
    генерации.
 * В итоге, при Back-off генерации, мы сможем всегда гарантированно получить
    варианты для генерации, поскольку у модели всегда есть униграммы, которые
    отвечают за конкретные буквы алфавита языка.

### Шаг 12.1. Объявление back-off N-грамм модели генерации текста

### Шаг 12.2. Генерация следующей буквы

Для генерации буквы нам необходимо: 
 * Контекст (последние `n-1` букв);
 * Распределение N-грамм в языке;
 * N-граммы разных размерностей.


Если в каждой из моделей не было найдено подходящего контекста, 
происходит переход к уни-граммам, которые должны быть собраны в
языковом профиле. В качестве генерируемой буквы выбирается самая частотная.


Если приходят некорректные значения, то возвращается значение `-1`.

**Пример**:

Есть модель с размером N-грамм равным 4, 
она уже обучена на тексте `'She is happy. He is happy.'`, 
выделены N-граммы, вычислены их частоты. 
Также были переданы обученные модели с размерами N-грамм 
1, 2, 3 и 4. Дан контекст: `(1, 6, 3)`, число слов: `1`. 
Необходимо по этому контексту сгенерировать предложение
из одного слова соответственно.

1. Модель пытается найти 4-грамму, которая начинается на `(1, 6, 3)`,
    но не может ее найти, так как ее просто нет.
2. Модель пробует сузить контекст и ищет триграмму, которая
    начинается на `(6, 3)` - снова неудача.
3. Модель снова сужает контекст, теперь она ищет биграмму, которая
    начинается на контекст `(3,)` - такие присутствуют: `(3, 4): 2`,
    и `(3, 6): 2` - модель берет первую биграмму `(3, 4)`, так как 
    она окажется первая после сортировки.
4. Модель получает последовательность `(1, 6, 3, 4)`. Модель снова
    ищет 4-грамму с контекстом уже равным `(6, 3, 4)`. Такой 4-граммы
    снова нет.
5. Модель переходит на меньший контекст `(3, 4)`, ищет триграмму.
    Модель находит единственную частотную триграмму `(3, 4, 1)`.
6. Модель завершает генерацию последовательности `(1, 6, 3, 4, 1)`,
    поскольку найден символ `1`, обозначающий конец слова, а в параметрах
    мы указали генерацию одного слова.
    
Частотные распределения для текста из примера выше для вашей самопроверки:
```
{
    (1, 2, 3, 4): 1, (2, 3, 4, 1): 1, 
    (1, 5, 2, 1): 2, (1, 3, 6, 7): 2, 
    (3, 6, 7, 7): 2, (6, 7, 7, 8): 2, 
    (7, 7, 8, 1): 2, (1, 3, 4, 1): 1
}
{
    (1, 2, 3): 1, (2, 3, 4): 1, (3, 4, 1): 2, 
    (1, 5, 2): 2, (5, 2, 1): 2, (1, 3, 6): 2, 
    (3, 6, 7): 2, (6, 7, 7): 2, (7, 7, 8): 2, 
    (7, 8, 1): 2, (1, 3, 4): 1
}
{
    (1, 2): 1, (2, 3): 1, (3, 4): 2, (4, 1): 2, 
    (1, 5): 2, (5, 2): 2, (2, 1): 2, (1, 3): 3, 
    (3, 6): 2, (6, 7): 2, (7, 7): 2, (7, 8): 2, 
    (8, 1): 2
}
{
    (1,): 12, (2,): 3, (3,): 4, (4,): 2, 
    (5,): 2, (6,): 2, (7,): 4, (8,): 2
}
```

Идентификаторы в хранилище `LetterStorage`:
```py
{'_': 1, 's': 2, 'h': 3, 'e': 4, 'i': 5, 'a': 6, 'p': 7, 'y': 8}
```


### Шаг 13.1. Открыть и адаптировать публичный языковой профиль

### Шаг 14. Работа трех генераторов на экзотическом языке
